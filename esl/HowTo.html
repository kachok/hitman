<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head>


<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"/>
<title>Approve Rejected Assignments - Running the ApproveRejected script</title>
<!--
These style should be copied directly from the common CSS.
We include them in the document to make distributing files
easier.
-->


<style type="text/css">
/* --------------------------------------------- */
/*  AWS Developer Content Styles                 */
/* --------------------------------------------- */

body, div, p, td, th {
    font-family : helvetica,sans-serif, arial;
    font-size : 12px;
    color : #000000;
}

.aws-h1, h1{
  font-family: verdana, arial, helvetica, sans-serif;
  font-size: 18px;
  font-weight: bold;
  color: #000000;
}
.aws-h2, h2{
  font-family: verdana, arial, helvetica, sans-serif;
  font-size: 14px;
  color: #3366FF;
}
.aws-h3, h3{
  font-family: verdana, arial, helvetica, sans-serif;
  font-size: 12px;
  font-weight: bold;
  color: #333333;
}

/*table ID selector styles*/
#aws-table {
  border-top: 1px solid #cccccc;
  border-left: 1px solid #cccccc;
  width:95%;
}

#aws-table td{
  font-size: 12px;
  padding: 5px 5px 5px 5px;
  border-bottom: 1px solid #cccccc;
  border-right: 1px solid #cccccc;
  vertical-align:top;
}

#aws-table th {
  font-size: 12px;
  padding: 5px 5px 5px 5px;
  border-bottom: 1px solid #cccccc;
  border-right: 1px solid #cccccc;
  vertical-align:top;
  background-color: #eeeeee;
  color: #333333;
  font-size: 12px;
  font-weight: bold;
  text-align: left;
}

/*code, note, and example styles*/
.programlisting{
  font-family: "Courier New", Courier, mono;
  font-size: 12px;
  margin-top: 5px;
  margin-bottom: 5px;
  color: #000066;
  padding: 5px;
  background-color: #eff7ff;
  border: 1px dashed #333;
  width:75%;
  display:block;
}
.aws-code-inline {
  font-family: "Courier New", Courier, mono;
  font-size: 11px;
}
.aws-note {
  border-top-width: 1px;
  border-right-width: 1px;
  border-bottom-width: 1px;
  border-left-width: 1px;
  b
border-top-style: solid;
  border-right-style: none;
  border-bottom-style: solid;
  border-left-style: none;
  border-top-color: #CCCCCC;
  border-right-color: #CCCCCC;
  border-bottom-color: #CCCCCC;
  border-left-color: #CCCCCC;
  padding: 10px;
  width:95%;
  margin-left: 20px;
  margin-right: 20px;
}
</style></head><body>
<h1>ESL Error-Correction HIT Pipeline</h1>

<h2>Source files</h2>
<h3>Scripts</h3>
The esl/src directory contains python scripts for creating and posting HITs, retrieving data, grading quality control, and dumping data into csv format. Below is a list of the contained files and their descriptions.
<ul>
<li>add_esl_hits_to_mturk.py - add the HITs generated by generate_esl_hits.py to MTurk</li>
<li>approve_reject.py - read reviewed assignments from database (buffered in process_qc.py), approve/reject each assignment on MTurk, and roll buffered review data into the esl_workers database</li>
<li>buffer_update.py - take results out of buffer and read into relevant esl2 tables (should be run after running multi_test, since requires full buffer)</li>
<li>reloadall.sh - runs cleanup.py, cleardb.py, load_data_to_db.py in that order. recommended using instead of running individually, since it will avoid accidentally clearing DB before removing hits from mturk.</li>
<li>cleanup.py - expire all hits currently on mturk, make no longer available to workers</li>
<li>cleardb.py - clear all entries out of DB. clears all sentences, hits, assignments, hits_results, location data. does not clear esl_workers data or esl_edits data</li> 
<li>data_dump.py - pull all data out of the DB and dumps into csv files. creates 7 files (assign_data, edit_data, hitdata_data, worker_data, cntrl_data, hit_data, sent_ids) corresponding to the esl2 tables assignments, esl_edits, esl_hits_data, esl_workers, esl_controls, hits, and esl_sents)</li>
<li>generate_cntrlonly_hits.py - generate mturk hits consisting only of artifically-generated error sentences, does not post hits to mturk</li>
<li>generage_esl_hits.py -  generate ESL mturk hits, does not post hits to mturk</li>
<li>hit_dump_multi_test.py - pull all assignments from MTurk and dump raw values into csv file (hit_data_dump), for backup purposes</li>
<li>load_data_to_db.py - read sentences from file named in DATA_PATH (line 23) and add them to the esl_sentences data table, to be used for generating hits</li>
<li>mturk.py - methods for calling mturk api, using boto libraries</li>
<li>multi_test.py - pull all hits off of mturk and read into buffer_assignments table</li>
<li>process_qc.py - review control sentence for each submitted assignment, compute accuracy on that sentence, read and update worker-level statistics into esl_appr_buffer, determine whether assignment should be accepted or rejected, and enter accept/reject into esl_grades (does not do any actual accepting/rejected yet)</li>
<li>repost_hits.py - add assignments to HITs to replace assignments that were rejected for not meeting QC standards</li>
</ul>
<h3>Database</h3>
<ul>
<li>esl.sql - builds all sql tables needed to support esl hit generation, data collection, and grading.</li>
</ul>
<h3>Libraries</h3>
<ul>
<li>controls.py - methods for pulling control sentences off of wikipedia, using the titles of the foreign language wikipedia pages (right now, only in urdu).</li>
<li>edit_anal.py - methods for parsing edit data structures and organizing edits</li>
<li>edit_graph.py - definitions of edit, revision, and sentence objects, as well as graph data structures containing them</li>
<li>extract_data.py - functions for converting data in csv files into data structures defined in edit_graph, or other in-memory data structures</li>
<li>figures.py - methods for generating graphs and figures for data analysis (not used here, included to make above scripts compile. planning to clean up code and remove this file from this repo soon...)</li>	
<li>generrors.py - methods to introduce random errors into English sentences</li>
<li>qc.py - methods for grading control sentences and paying workers based on performance</li>
</ul>

<h2>Running the pipeline</h2>
<h3>Creating and posting HITs</h3>
Run the following once per HIT batch:
<ul>
<li>sh reload_all.sh </li>
<li>python generate_esl_hits.py (--reload if want to re-query for all control sentences, otherwise controls from last run will be used. must be run with --reload on first run, since --reload will build the controls.log file which will be used in future runs.)</li>
<li>python add_esl_hits_to_mturk.py</li> 
</ul>
<h3>Retrieving data and grading HITs</h3>
Run the following periodically while HITs are still outstanding:
<ul>
<li>python multitest.py</li>
<li>python buffer_update.py</li>
<li>python process_qc.py</li>
<li>confirm approvals/rejections using apprej.log and DB tables</li>
<li>python approve_reject.py</li>
<li>python repost_hits.py</li>
</ul>

<h2>Pipeline design and implementation decisions</h2>
<h3>Source sentences for HIT</h3>
	<h4>Overview</h4>
<p>Our ESL correction HITs were conceived as part of a translation pipeline, so that the HITs take as input data the output data of a previously completed translation task. The output data from this translation HIT is available as part of the Indian Languages Corpus (git link). The translation task asked Turkers to translate into English sentences taken from a variety of wikipedia pages written in one of several Indian languages (citation here). The translated sentences, which contain grammatical and stylistic errors, are the input sentences to the ESL HIT. Since the translated sentences were pulled from wikipedia, the sentences are easily grouped into sets based on their source documents, so that translated sentences from the same source document are logically connected to one another. Documents were assigned a unique document id number, and each sentence in the database is stored with its document id, which is used during the HIT generation.</p>
 
	<h4>Technical details</h4>
<p>The input sentences to our HIT are stored in the esl_sentences table with the following schema:<br>
 id           | integer           | <br> 
 sentence     | character varying | <br> 
 sequence_num | integer           | <br>
 language_id  | integer           | <br>
 doc_id       | character varying | <br>
 qc           | integer           | <br>
 doc          | character varying | <br>
The doc_id has the format [document number]_[sentence number], where the document number uniquely identifies the original wikipedia page from which the sentence was taken and the sentence number (not relevant for this HIT) indentifies the specific sentence in that document. The doc column redundantly provides just the document id for the sentence. Both sequence_num and language_id are unused by this HIT: the language id identifes the language of the sentence (English for all of our input) and sequence_num identifies the order in which it was added to our database (used previously for batching sentences for HITs, but has been replaced by doc_id). The qc column is 1 if the sentence is a control sentence. Control sentences are added to the esl_sentences data table during HIT generation, and will be discussed in detail in the Quality Control section of the report. </p>
<p>So far, our HIT has taken sentences only from the Urdu-English dataset of the Indian languages corpus. The corpus provides 4 English translations of each original Urdu sentence. Because of the poor quality  and significant ambiguity of many of the translations, we proprocessed sentences and included only the 'best' sentence from each group of four, as determined by a translation-rating HIT (citation). In the translation-rating HIT, turkers were presented with 4 parallel translations of the same urdu sentence and asked to choose the best one (cast a 'vote' for the best translation). We take the 'best' translation to be the translation that receives the highest number of votes, with ties going to whichever sentences was listed first. After reducing our data set to only the best translations, we process the sentences to eliminate sentences with too few (&lt; 5) or too many (&gt; 15) words. This leaves us with approximately 5400 error-ful English senteces.</p>

<h3>HIT Generation</h3>
	<h4>Overview</h4>
<p>We present workers with 5 sentences per HIT, 4 of which are translations with unknown errors and one of which is a control with known errors. The choice of 5 sentences was partially due to the difficulty of the task (10 sentences could become lengthy and tedious based on the quality of the translations) and partially due to server restrictions (sending data for larger numbers of sentences sometimes caused a URI overflow error). Turkers were paid 5 cents per sentence, or 25 cents per HIT.</p>
<p>Because of the frequency of ambiguous translations, we chose to group sentences by document within each HIT, so that all the sentences presented to the Turker within a HIT come from the same document and relate to the same topic, rather than choose sentences for each HIT at random. Our hope was that seeing a group of related sentences together would help Turkers understand translations that were more ambiguous or had more serious errors.</p>		
	<h4>Technical details</h4>
<p>Generating HITs involves 3 tables in the database: hits, esl_sentences, esl_hits_data. Descriptions of these tables are as follows:<br>
hits:        
<br>mturk_hit_id - the unique alphanumeric identifier of this HIT, provided by Mturk 
<br>uuid - a unique key given to MTurk, require to create the HIT         
<br>hittype_id - integer identifying the HIT type 
<br>language_id - integer identifying the language
<br>assignments - number of assignments per HIT
<br>rejected - number of assignments rejected from this HIT
<br>approved  - number of assignments approved from this HIT
<br>status - whether the HIT is Open or Submitted (unused)<br>
esl_hits_data: contains 5 entries per HIT, one for each sentence
<br>hit_id - the hit in the hits table (id column of hits) 
<br>output - unused        
<br>data_quality - unused
<br>language_id - integer identifying the language
<br>sentence_num - order number (0 - 4) of this sentence in the HIT (i.e. where it appeared in the list of 5 sentences)
<br>esl_sentence_id - number identifying this sentence in the esl_sentences table<br>
</p>
<p>Each HIT consists of 5 sentences: 4 translations and one control. To generate the HITs, we pull all the sentences from the esl_sentences table, sort by document id, and batch them into groups of four sentences. For each group, we generate a control based on the four sentences (see Quality control section), which provides the fifth sentence. One entry, corresponding to the specific HIT, is added to the hits table; at this point, the mturk_hit_id column is left blank, do be filled later when the HIT is actually posted to MTurk. The set of five sentences is added to the esl_hits_data table, with one entry corresponding to each sentence that will exist in this HIT.</p>

<h3>Quality control</h3>
Our current approach to quality control attempts to automatically grade HITs. This is a work in progress, and may be replaced by human-grading, acheived through a second HIT. 	
<h4>Choosing English sentences</h4>
<h5>Overview</h5>
Since the sentences in our HITs are grouped by document, the controls need to be chosen to match the context of the other sentence, so as not to be easily identifiable. We choose our controls by finding the English wikipedia page which discusses the same topic as the Urdu page from which we pulled our sentence. We then use the tf*idf score to find the sentence from the English page which best matches the four translation sentences with which it will appear in the HIT.
<h5>Technical details</h5>
We use the wikipydia library to interface with the Wikipedia API using Python. We choose an English sentence to match the translated sentences as follows:
<p>Given a set of four translations, we take the document ID of those translations. In the case that the sentences come from more than one document (when the block of four sentences drawn from the database bridged a gap between two documents) we take the document ID of the first sentence. We query wikipedia for the interlanguage links that exist for that document ID. (The interlanguage links are the pages which contain the same title (same topic) but are in a different language; they are not translations of the original page.) If the page contains an English language link, we pull the text of the English page.</p>
<p>To ensure that only true sentences are selected as control sentences, we iterate through each sentence in the English text and eliminate sentences that do not contain both a noun and a verb. We attempted to do a more robust syntax parsing, but all phrases were given "Sentence" as their root by default, which made differentiation between complete sentences and fragments impossible. Using a simpler check for nouns and verbs was sufficient for our purposes, and successfully elminated most citations and titles that appear on Wikipedia pages.</p>
<p>For each sentence in remaing English text, we calculate a tf*idf score, using term frequencies calculated from the 4 provided translations with which we are trying to match context and document frequencies calculated across all translations over all HITs. We choose the sentence with the highest score, eliminating sentences that are not to long (&gt; 30 words) or too short (&lt; 5 words).</p>
	<h4>Introducing errors</h4>
<h5>Overview</h5>
 We randomly introduce errors into the choosen control sentence, confining the errors to those involving spelling, prepositions, determiners, and verbs. We experimented with two hueristics: 1) A random number of errors chosen based on the length of the sentence and 2) a fixed three errors per sentence. The former is the implementation currently in use, which can produce complicated sentences with too many errors. We have not yet implemented or collected results for the latter, so only the former will be discussed below. We store a record of the introduced errors to be used during the grading of the HIT (see Grading section below). 
<h5>Technical details</h5>
<p>We choose the number of errors to be introduced into the sentence as a random number, no greater than one third of the number of words in the sentence, and with a hard maximum at 5 errors. The maximums are enforced to ensure that the sentence is still comprehendable to the reader. We parse and POS-tag the sentence. We then introduce at random errors of the following types:
<ul>

<li>Spelling: A word (with length &gt; 4 characters) is chosen at random. One letter from the word is chosen at random (excluding the first letter). The chosen letter is swapped with the letter immediately following. Ex. German <b>forces</b> surrendered in Italy on 29 April.-> German <b>forecs</b> surrendered in an Italy on 29 April .</li>

<li>Prepositions: A preposition is chosen at random. In is randomly exchanged with one of the following prepositions: {in, on, at, of, for, with, by}. Note that some prepositions not included in the list may still be chosen and switched. E.x. 'before' -> 'on'.Ex. German forces surrendered in Italy <b>on</b> 29 April.-> German forces surrenders in Italy <b>of</b> 29 Apirl .</li>

<li>Verbs: Our current implementation does not handle irregular uniquely, except in the case of "to be". This is going to change for our next batch of HITs, and this file will be updated accordingly. Currently, our procedure is as follows:
<br>A verb is chosen at random. If the verb is a form of "to be" (in the set {'be', 'is', 'am', 'are', 'was', 'were', 'being'}) it is randomly exchanged with another verb in this set. If the verb is not a form of "to be", it's ending is identified as one of the following forms: {'ing', 'ed', 'es', 's', none}. The ending is randomly exchanged with another ending. <br>E.x. German forces <b>surrendered</b> in Italy on 29 April.-> German forces <b>surrenders</b> in Italy of 29 Apirl .</li>

<li>Determiners: Determiner errors involve random insertion, deletion, or substitution of determiners. <br>Insertions only happen in front of noun phrases, do avoid unatural errors such as "United the Nations." For insertions, a noun phrase (which is not currently preceded by a determiner) is chosen at random. A determiner is randomly chosen from the set {a, the, an}, and inserted directly before the chosen noun phrase.
<br>For deletions, a determiner is deleted at random.
<br>For substitutions, a determiner is chosen at random, and exchanged with one of the determiners in the set mentioned above. Note that some determiners not included in the list may still be chosen and switched. E.x. 'those' -> 'an'. E.x. German forces surrendered in Italy on 29 April.-> German forecs surrendered in <b>an</b> Italy on 29 April .  </li>
</li></ul>
<p>
We store each introduced error in the esl_control database, which contains the following columns:
<br>esl_sentence_id: the id in the esl_sentences table where this errorful sentence (final version as it appears to the Turker) is stored.
<br>sentence: the original, error-free sentence, stored in full text for convenience.   
<br>err_idx: the index where the error is introduced       
<br>oldwd: the word as it appeared originally          
<br>newwd: the word as it appears after the error was introduced          
<br>mode: the mode (insert, delete, change) by which the error <b>should be corrected</b> i.e. if a determiner "the" is inserted into a sentence, it will be stored as mode=delete.
<br>hit_id: the hit in which the control with this error appears         
<br>seq_num: the number representing when in order this error was introduced, relative to the other errors in this sentence.
<br> For more information on error representation and storage, see Data Representation and Storage.

<h4>Grading</h4>
<h5>Overview</h5>
Our current implementation attempts to grade control sentences automatically. We look at the Turker's submitted, corrected version of our control sentence, compare the indicies of the known errors with the corresponding indicies in our original sentence, and compute an average number of errors corrected. The details of this algorith are given below. This method has not provided the reliability we had hoped it would, due to wider than expected variability in Turkers' corrections. We are beginning word on a Turker-graded quality control method, in which the quality of work in this HIT is graded by workers in another HIT. Documentation will be updated to included information about the Quality Control HIT after it has been implemented.
<h5>Technical details</h5>
See Data Representation and Storage for information about the data structures used for the below algorithms. To grade the 


<h2>Data Representation and Storage<h2>
<h3>Database schema</h3>
Information about the DB.
<h3>Error Representation</h3>
Both introduced errors (made while creating controls) and edits (submitted by the Turkers) are represented by the same pieces of information, to allow for reuse of data structures and simplified grading of controls. Both with be referred to as an 'edit' for convenience. An edit within a sentence is defined by the following:
span_start - the index at which the edit begins (inclusive)
span_end - the index at which the edit ends (not inclusive)
new_word - the text of the word after the edit 
old_word - the text of the word before the edit (for convenience and debugging)
edit_type - either change, delete, insert, or move, based on how the edit was made
seq_num - the sequence number representing when the edit was made, relative to other edits on the same sentence, so that edits sorted by increasing seq_num will be in the same order in which they were made.

<h3>Sentence and Edit Data Structures</h3>
<h3>Input and output data formats</h3>
Information about the data.




